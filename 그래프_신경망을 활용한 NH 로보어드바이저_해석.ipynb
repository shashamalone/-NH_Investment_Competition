{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load S&P500 ticker list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Using cached yfinance-0.2.30-py2.py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\rladl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (2022.6)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\rladl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (4.11.1)\n",
      "Collecting peewee>=3.16.2\n",
      "  Using cached peewee-3.16.3.tar.gz (928 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting multitasking>=0.0.7\n",
      "  Using cached multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Collecting requests>=2.31\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\rladl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (4.9.1)\n",
      "Collecting html5lib>=1.1\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Collecting appdirs>=1.4.4\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\rladl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (1.5.1)\n",
      "Collecting frozendict>=2.3.4\n",
      "  Downloading frozendict-2.3.8-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\rladl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (1.23.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rladl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.2.post1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\rladl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\rladl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\rladl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rladl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31->yfinance) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rladl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31->yfinance) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rladl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31->yfinance) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rladl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31->yfinance) (2022.9.24)\n",
      "Building wheels for collected packages: peewee\n",
      "  Building wheel for peewee (pyproject.toml): started\n",
      "  Building wheel for peewee (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for peewee: filename=peewee-3.16.3-py3-none-any.whl size=135547 sha256=76371820982acf0c775194db6cbb4e6cb1217f8d9fedb80cc1360508acefb98c\n",
      "  Stored in directory: c:\\users\\rladl\\appdata\\local\\pip\\cache\\wheels\\96\\8b\\f5\\9380082664c2b4b5c193675629393a822974623e5d9ddbb0b7\n",
      "Successfully built peewee\n",
      "Installing collected packages: peewee, multitasking, appdirs, requests, html5lib, frozendict, yfinance\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "Successfully installed appdirs-1.4.4 frozendict-2.3.8 html5lib-1.1 multitasking-0.0.11 peewee-3.16.3 requests-2.31.0 yfinance-0.2.30\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data library load\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf # Yahoo Finance(금융 및 주식)의 데이터를 쉽게 가져올 수 있도록 도와주는 라이브러리\n",
    "import time\n",
    "from glob import glob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'../data' 경로에 디렉토리를 생성하는 함수 : 디렉토리가 존재하지 않을 시 디렉토리 형성\n",
    " \n",
    "def create_dir(dir_path):\n",
    "    if os.path.isdir(dir_path) is False: os.makedirs(dir_path)\n",
    "\n",
    "create_dir('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 가지 다른 버전의 S&P 500 종목 목록을 비교하여 공통된 종목을 찾는 것\n",
    "\n",
    "# S&P 500 지수 구성 종목의 목록을 웹에서 가져옴\n",
    "#해당 목록을 데이터프레임에 저장하는 작업을 수행\n",
    "\n",
    "# get sp500 list in valid\n",
    "# s&p 500 지수의 유효한(valid) 및 테스트(test) 버전의 url 저장\n",
    "valid_start_sp500_url = 'https://en.wikipedia.org/w/index.php?title=List_of_S%26P_500_companies&oldid=1095558369'\n",
    "\n",
    "# get sp500 list in test\n",
    "test_end_sp500_url = 'https://en.wikipedia.org/w/index.php?title=List_of_S%26P_500_companies&oldid=1173149676'\n",
    "\n",
    "# get ticker list for modeling\n",
    "# html 데이터 -> 테이터 프레임으로 저장 \n",
    "# flavor='bs4'는 HTML 파서로 Beautiful Soup 4를 사용\n",
    "# pd.read_html 함수는 웹 페이지에서 테이블을 스크래핑하여 데이터프레임으로 반환 -> 첫번째 테이블 선택\n",
    "df_valid_start = pd.read_html(valid_start_sp500_url, flavor='bs4')[0] # get sp500 page table\n",
    "df_test_end = pd.read_html(valid_start_sp500_url, flavor='bs4')[0] # get sp500 page table\n",
    "wiki_set = set(df_valid_start['Symbol'].values) & set(df_test_end['Symbol'].values) # remain tickers\n",
    "    # df_valid_start& df_test_end 데이터프레임에서 'Symbol' 열의 값 -> (set)의 교집합 찾음\n",
    "    # df_valid_start와 df_test_end에 모두 포함된 S&P 500 종목의 심볼(Symbol)을 구함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['NLOK']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['FRC']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['PKI']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['DRE']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['NLSN']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['CTXS']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['SIVB']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['RE']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['TWTR']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['FISV']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['FBHS']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# load tickers prices\n",
    "# Yahoo Finance API를 사용하여 주식 종목의 가격 데이터를 가져오고 이를 CSV 파일로 저장하는 함수\n",
    "\n",
    "def load_tickers_prices():\n",
    "  \n",
    "    apple_god = yf.download('AAPL', start='2021-07-01', end='2023-09-01') # get absolute date index\n",
    "        # 'AAPL' 심볼에 대한 주가 데이터를 가져와서 'apple_god' 변수에 저장\n",
    "        # 이 데이터는 2021년 7월 1일부터 2023년 9월 1일까지의 주가 데이터를 포함 \n",
    "    tickers_dir = 'data/ticker'#디렉토리 경로지정\n",
    "    create_dir(tickers_dir) #위에서 설정한 디렉토리 경로 지정하는 함수 사용\n",
    "    for ticker in list(wiki_set): # 'wiki_set'에 있는 S&P 500 종목의 심볼(Symbol) 목록을 반복\n",
    "        if '.' in ticker: ticker = ticker.replace('.', '-') # processing BF.B & BRK.B \n",
    "            \n",
    "        try:\n",
    "            df = yf.download(ticker, start='2021-07-01', end='2023-09-01', timeout=15) # get OHLCV dataframe of tickers\n",
    "                # Yahoo Finance API를 사용하여 'ticker'에 해당하는 주식의 OHLCV(Open, High, Low, Close, Volume) 데이터를 가져옴 \n",
    "                # 'timeout' 매개변수는 데이터를 다운로드하는 데 최대 15초까지 대기하도록 설정\n",
    "            df.dropna(inplace=True)\n",
    "            if len(df) != len(apple_god): continue # worship\n",
    "                #  현재 종목의 데이터 길이가 'apple_god' 변수에 저장된 'AAPL' 데이터 길이와 다르다면, \n",
    "                #  이 종목은 건너뛰고 다음 종목으로 진행합니다. \n",
    "                #  이렇게 함으로써, 'AAPL'과 동일한 기간 동안 데이터를 갖고 있는 종목만을 선택\n",
    "            df.to_csv(f'{tickers_dir}/{ticker}.csv')\n",
    "            time.sleep(1)\n",
    "                #time.sleep(1): 데이터를 다운로드한 후 1초의 딜레이를 추가\n",
    "                #이것은 너무 빠른 요청을 방지\n",
    "        except:\n",
    "            continue\n",
    "                # 어떤 예외가 발생하더라도 해당 종목을 건너뛰고 다음 종목으로 계속 진행 \n",
    "\n",
    "load_tickers_prices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ticker\\\\A',\n",
       " 'ticker\\\\AAL',\n",
       " 'ticker\\\\AAP',\n",
       " 'ticker\\\\AAPL',\n",
       " 'ticker\\\\ABBV',\n",
       " 'ticker\\\\ABC',\n",
       " 'ticker\\\\ABT',\n",
       " 'ticker\\\\ACN',\n",
       " 'ticker\\\\ADBE',\n",
       " 'ticker\\\\ADI',\n",
       " 'ticker\\\\ADM',\n",
       " 'ticker\\\\ADP',\n",
       " 'ticker\\\\ADSK',\n",
       " 'ticker\\\\AEE',\n",
       " 'ticker\\\\AEP',\n",
       " 'ticker\\\\AES',\n",
       " 'ticker\\\\AFL',\n",
       " 'ticker\\\\AIG',\n",
       " 'ticker\\\\AIZ',\n",
       " 'ticker\\\\AJG',\n",
       " 'ticker\\\\AKAM',\n",
       " 'ticker\\\\ALB',\n",
       " 'ticker\\\\ALGN',\n",
       " 'ticker\\\\ALK',\n",
       " 'ticker\\\\ALL',\n",
       " 'ticker\\\\ALLE',\n",
       " 'ticker\\\\AMAT',\n",
       " 'ticker\\\\AMCR',\n",
       " 'ticker\\\\AMD',\n",
       " 'ticker\\\\AME',\n",
       " 'ticker\\\\AMGN',\n",
       " 'ticker\\\\AMP',\n",
       " 'ticker\\\\AMT',\n",
       " 'ticker\\\\AMZN',\n",
       " 'ticker\\\\ANET',\n",
       " 'ticker\\\\ANSS',\n",
       " 'ticker\\\\AON',\n",
       " 'ticker\\\\AOS',\n",
       " 'ticker\\\\APA',\n",
       " 'ticker\\\\APD',\n",
       " 'ticker\\\\APH',\n",
       " 'ticker\\\\APTV',\n",
       " 'ticker\\\\ARE',\n",
       " 'ticker\\\\ATO',\n",
       " 'ticker\\\\ATVI',\n",
       " 'ticker\\\\AVB',\n",
       " 'ticker\\\\AVGO',\n",
       " 'ticker\\\\AVY',\n",
       " 'ticker\\\\AWK',\n",
       " 'ticker\\\\AXP',\n",
       " 'ticker\\\\AZO',\n",
       " 'ticker\\\\BA',\n",
       " 'ticker\\\\BAC',\n",
       " 'ticker\\\\BALL',\n",
       " 'ticker\\\\BAX',\n",
       " 'ticker\\\\BBWI',\n",
       " 'ticker\\\\BBY',\n",
       " 'ticker\\\\BDX',\n",
       " 'ticker\\\\BEN',\n",
       " 'ticker\\\\BF-B',\n",
       " 'ticker\\\\BIIB',\n",
       " 'ticker\\\\BIO',\n",
       " 'ticker\\\\BK',\n",
       " 'ticker\\\\BKNG',\n",
       " 'ticker\\\\BKR',\n",
       " 'ticker\\\\BLK',\n",
       " 'ticker\\\\BMY',\n",
       " 'ticker\\\\BR',\n",
       " 'ticker\\\\BRK-B',\n",
       " 'ticker\\\\BRO',\n",
       " 'ticker\\\\BSX',\n",
       " 'ticker\\\\BWA',\n",
       " 'ticker\\\\BXP',\n",
       " 'ticker\\\\C',\n",
       " 'ticker\\\\CAG',\n",
       " 'ticker\\\\CAH',\n",
       " 'ticker\\\\CARR',\n",
       " 'ticker\\\\CAT',\n",
       " 'ticker\\\\CB',\n",
       " 'ticker\\\\CBOE',\n",
       " 'ticker\\\\CBRE',\n",
       " 'ticker\\\\CCI',\n",
       " 'ticker\\\\CCL',\n",
       " 'ticker\\\\CDAY',\n",
       " 'ticker\\\\CDNS',\n",
       " 'ticker\\\\CDW',\n",
       " 'ticker\\\\CE',\n",
       " 'ticker\\\\CF',\n",
       " 'ticker\\\\CFG',\n",
       " 'ticker\\\\CHD',\n",
       " 'ticker\\\\CHRW',\n",
       " 'ticker\\\\CHTR',\n",
       " 'ticker\\\\CI',\n",
       " 'ticker\\\\CINF',\n",
       " 'ticker\\\\CL',\n",
       " 'ticker\\\\CLX',\n",
       " 'ticker\\\\CMA',\n",
       " 'ticker\\\\CMCSA',\n",
       " 'ticker\\\\CME',\n",
       " 'ticker\\\\CMG',\n",
       " 'ticker\\\\CMI',\n",
       " 'ticker\\\\CMS',\n",
       " 'ticker\\\\CNC',\n",
       " 'ticker\\\\CNP',\n",
       " 'ticker\\\\COF',\n",
       " 'ticker\\\\COO',\n",
       " 'ticker\\\\COP',\n",
       " 'ticker\\\\COST',\n",
       " 'ticker\\\\CPB',\n",
       " 'ticker\\\\CPRT',\n",
       " 'ticker\\\\CPT',\n",
       " 'ticker\\\\CRL',\n",
       " 'ticker\\\\CRM',\n",
       " 'ticker\\\\CSCO',\n",
       " 'ticker\\\\CSX',\n",
       " 'ticker\\\\CTAS',\n",
       " 'ticker\\\\CTLT',\n",
       " 'ticker\\\\CTRA',\n",
       " 'ticker\\\\CTSH',\n",
       " 'ticker\\\\CTVA',\n",
       " 'ticker\\\\CVS',\n",
       " 'ticker\\\\CVX',\n",
       " 'ticker\\\\CZR',\n",
       " 'ticker\\\\D',\n",
       " 'ticker\\\\DAL',\n",
       " 'ticker\\\\DD',\n",
       " 'ticker\\\\DE',\n",
       " 'ticker\\\\DFS',\n",
       " 'ticker\\\\DG',\n",
       " 'ticker\\\\DGX',\n",
       " 'ticker\\\\DHI',\n",
       " 'ticker\\\\DHR',\n",
       " 'ticker\\\\DIS',\n",
       " 'ticker\\\\DISH',\n",
       " 'ticker\\\\DLR',\n",
       " 'ticker\\\\DLTR',\n",
       " 'ticker\\\\DOV',\n",
       " 'ticker\\\\DOW',\n",
       " 'ticker\\\\DPZ',\n",
       " 'ticker\\\\DRI',\n",
       " 'ticker\\\\DTE',\n",
       " 'ticker\\\\DUK',\n",
       " 'ticker\\\\DVA',\n",
       " 'ticker\\\\DVN',\n",
       " 'ticker\\\\DXC',\n",
       " 'ticker\\\\DXCM',\n",
       " 'ticker\\\\EA',\n",
       " 'ticker\\\\EBAY',\n",
       " 'ticker\\\\ECL',\n",
       " 'ticker\\\\ED',\n",
       " 'ticker\\\\EFX',\n",
       " 'ticker\\\\EIX',\n",
       " 'ticker\\\\EL',\n",
       " 'ticker\\\\ELV',\n",
       " 'ticker\\\\EMN',\n",
       " 'ticker\\\\EMR',\n",
       " 'ticker\\\\ENPH',\n",
       " 'ticker\\\\EOG',\n",
       " 'ticker\\\\EPAM',\n",
       " 'ticker\\\\EQIX',\n",
       " 'ticker\\\\EQR',\n",
       " 'ticker\\\\ES',\n",
       " 'ticker\\\\ESS',\n",
       " 'ticker\\\\ETN',\n",
       " 'ticker\\\\ETR',\n",
       " 'ticker\\\\ETSY',\n",
       " 'ticker\\\\EVRG',\n",
       " 'ticker\\\\EW',\n",
       " 'ticker\\\\EXC',\n",
       " 'ticker\\\\EXPD',\n",
       " 'ticker\\\\EXPE',\n",
       " 'ticker\\\\EXR',\n",
       " 'ticker\\\\F',\n",
       " 'ticker\\\\FANG',\n",
       " 'ticker\\\\FAST',\n",
       " 'ticker\\\\FCX',\n",
       " 'ticker\\\\FDS',\n",
       " 'ticker\\\\FDX',\n",
       " 'ticker\\\\FE',\n",
       " 'ticker\\\\FFIV',\n",
       " 'ticker\\\\FIS',\n",
       " 'ticker\\\\FITB',\n",
       " 'ticker\\\\FLT',\n",
       " 'ticker\\\\FMC',\n",
       " 'ticker\\\\FOX',\n",
       " 'ticker\\\\FOXA',\n",
       " 'ticker\\\\FRT',\n",
       " 'ticker\\\\FTNT',\n",
       " 'ticker\\\\FTV',\n",
       " 'ticker\\\\GD',\n",
       " 'ticker\\\\GE',\n",
       " 'ticker\\\\GILD',\n",
       " 'ticker\\\\GIS',\n",
       " 'ticker\\\\GL',\n",
       " 'ticker\\\\GLW',\n",
       " 'ticker\\\\GM',\n",
       " 'ticker\\\\GNRC',\n",
       " 'ticker\\\\GOOG',\n",
       " 'ticker\\\\GOOGL',\n",
       " 'ticker\\\\GPC',\n",
       " 'ticker\\\\GPN',\n",
       " 'ticker\\\\GRMN',\n",
       " 'ticker\\\\GS',\n",
       " 'ticker\\\\GWW',\n",
       " 'ticker\\\\HAL',\n",
       " 'ticker\\\\HAS',\n",
       " 'ticker\\\\HBAN',\n",
       " 'ticker\\\\HCA',\n",
       " 'ticker\\\\HD',\n",
       " 'ticker\\\\HES',\n",
       " 'ticker\\\\HIG',\n",
       " 'ticker\\\\HII',\n",
       " 'ticker\\\\HLT',\n",
       " 'ticker\\\\HOLX',\n",
       " 'ticker\\\\HON',\n",
       " 'ticker\\\\HPE',\n",
       " 'ticker\\\\HPQ',\n",
       " 'ticker\\\\HRL',\n",
       " 'ticker\\\\HSIC',\n",
       " 'ticker\\\\HST',\n",
       " 'ticker\\\\HSY',\n",
       " 'ticker\\\\HUM',\n",
       " 'ticker\\\\HWM',\n",
       " 'ticker\\\\IBM',\n",
       " 'ticker\\\\ICE',\n",
       " 'ticker\\\\IDXX',\n",
       " 'ticker\\\\IEX',\n",
       " 'ticker\\\\IFF',\n",
       " 'ticker\\\\ILMN',\n",
       " 'ticker\\\\INCY',\n",
       " 'ticker\\\\INTC',\n",
       " 'ticker\\\\INTU',\n",
       " 'ticker\\\\IP',\n",
       " 'ticker\\\\IPG',\n",
       " 'ticker\\\\IQV',\n",
       " 'ticker\\\\IR',\n",
       " 'ticker\\\\IRM',\n",
       " 'ticker\\\\ISRG',\n",
       " 'ticker\\\\IT',\n",
       " 'ticker\\\\ITW',\n",
       " 'ticker\\\\IVZ',\n",
       " 'ticker\\\\J',\n",
       " 'ticker\\\\JBHT',\n",
       " 'ticker\\\\JCI',\n",
       " 'ticker\\\\JKHY',\n",
       " 'ticker\\\\JNJ',\n",
       " 'ticker\\\\JNPR',\n",
       " 'ticker\\\\JPM',\n",
       " 'ticker\\\\K',\n",
       " 'ticker\\\\KDP',\n",
       " 'ticker\\\\KEY',\n",
       " 'ticker\\\\KEYS',\n",
       " 'ticker\\\\KHC',\n",
       " 'ticker\\\\KIM',\n",
       " 'ticker\\\\KLAC',\n",
       " 'ticker\\\\KMB',\n",
       " 'ticker\\\\KMI',\n",
       " 'ticker\\\\KMX',\n",
       " 'ticker\\\\KO',\n",
       " 'ticker\\\\KR',\n",
       " 'ticker\\\\L',\n",
       " 'ticker\\\\LDOS',\n",
       " 'ticker\\\\LEN',\n",
       " 'ticker\\\\LH',\n",
       " 'ticker\\\\LHX',\n",
       " 'ticker\\\\LIN',\n",
       " 'ticker\\\\LKQ',\n",
       " 'ticker\\\\LLY',\n",
       " 'ticker\\\\LMT',\n",
       " 'ticker\\\\LNC',\n",
       " 'ticker\\\\LNT',\n",
       " 'ticker\\\\LOW',\n",
       " 'ticker\\\\LRCX',\n",
       " 'ticker\\\\LUMN',\n",
       " 'ticker\\\\LUV',\n",
       " 'ticker\\\\LVS',\n",
       " 'ticker\\\\LW',\n",
       " 'ticker\\\\LYB',\n",
       " 'ticker\\\\LYV',\n",
       " 'ticker\\\\MA',\n",
       " 'ticker\\\\MAA',\n",
       " 'ticker\\\\MAR',\n",
       " 'ticker\\\\MAS',\n",
       " 'ticker\\\\MCD',\n",
       " 'ticker\\\\MCHP',\n",
       " 'ticker\\\\MCK',\n",
       " 'ticker\\\\MCO',\n",
       " 'ticker\\\\MDLZ',\n",
       " 'ticker\\\\MDT',\n",
       " 'ticker\\\\MET',\n",
       " 'ticker\\\\META',\n",
       " 'ticker\\\\MGM',\n",
       " 'ticker\\\\MHK',\n",
       " 'ticker\\\\MKC',\n",
       " 'ticker\\\\MKTX',\n",
       " 'ticker\\\\MLM',\n",
       " 'ticker\\\\MMC',\n",
       " 'ticker\\\\MMM',\n",
       " 'ticker\\\\MNST',\n",
       " 'ticker\\\\MO',\n",
       " 'ticker\\\\MOH',\n",
       " 'ticker\\\\MOS',\n",
       " 'ticker\\\\MPC',\n",
       " 'ticker\\\\MPWR',\n",
       " 'ticker\\\\MRK',\n",
       " 'ticker\\\\MRNA',\n",
       " 'ticker\\\\MRO',\n",
       " 'ticker\\\\MS',\n",
       " 'ticker\\\\MSCI',\n",
       " 'ticker\\\\MSFT',\n",
       " 'ticker\\\\MSI',\n",
       " 'ticker\\\\MTB',\n",
       " 'ticker\\\\MTCH',\n",
       " 'ticker\\\\MTD',\n",
       " 'ticker\\\\MU',\n",
       " 'ticker\\\\NCLH',\n",
       " 'ticker\\\\NDAQ',\n",
       " 'ticker\\\\NDSN',\n",
       " 'ticker\\\\NEE',\n",
       " 'ticker\\\\NEM',\n",
       " 'ticker\\\\NFLX',\n",
       " 'ticker\\\\NI',\n",
       " 'ticker\\\\NKE',\n",
       " 'ticker\\\\NOC',\n",
       " 'ticker\\\\NOW',\n",
       " 'ticker\\\\NRG',\n",
       " 'ticker\\\\NSC',\n",
       " 'ticker\\\\NTAP',\n",
       " 'ticker\\\\NTRS',\n",
       " 'ticker\\\\NUE',\n",
       " 'ticker\\\\NVDA',\n",
       " 'ticker\\\\NVR',\n",
       " 'ticker\\\\NWL',\n",
       " 'ticker\\\\NWS',\n",
       " 'ticker\\\\NWSA',\n",
       " 'ticker\\\\NXPI',\n",
       " 'ticker\\\\O',\n",
       " 'ticker\\\\ODFL',\n",
       " 'ticker\\\\OGN',\n",
       " 'ticker\\\\OKE',\n",
       " 'ticker\\\\OMC',\n",
       " 'ticker\\\\ON',\n",
       " 'ticker\\\\ORCL',\n",
       " 'ticker\\\\ORLY',\n",
       " 'ticker\\\\OTIS',\n",
       " 'ticker\\\\OXY',\n",
       " 'ticker\\\\PARA',\n",
       " 'ticker\\\\PAYC',\n",
       " 'ticker\\\\PAYX',\n",
       " 'ticker\\\\PCAR',\n",
       " 'ticker\\\\PEAK',\n",
       " 'ticker\\\\PEG',\n",
       " 'ticker\\\\PENN',\n",
       " 'ticker\\\\PEP',\n",
       " 'ticker\\\\PFE',\n",
       " 'ticker\\\\PFG',\n",
       " 'ticker\\\\PG',\n",
       " 'ticker\\\\PGR',\n",
       " 'ticker\\\\PH',\n",
       " 'ticker\\\\PHM',\n",
       " 'ticker\\\\PKG',\n",
       " 'ticker\\\\PLD',\n",
       " 'ticker\\\\PM',\n",
       " 'ticker\\\\PNC',\n",
       " 'ticker\\\\PNR',\n",
       " 'ticker\\\\PNW',\n",
       " 'ticker\\\\POOL',\n",
       " 'ticker\\\\PPG',\n",
       " 'ticker\\\\PPL',\n",
       " 'ticker\\\\PRU',\n",
       " 'ticker\\\\PSA',\n",
       " 'ticker\\\\PSX',\n",
       " 'ticker\\\\PTC',\n",
       " 'ticker\\\\PVH',\n",
       " 'ticker\\\\PWR',\n",
       " 'ticker\\\\PXD',\n",
       " 'ticker\\\\PYPL',\n",
       " 'ticker\\\\QCOM',\n",
       " 'ticker\\\\QRVO',\n",
       " 'ticker\\\\RCL',\n",
       " 'ticker\\\\REG',\n",
       " 'ticker\\\\REGN',\n",
       " 'ticker\\\\RF',\n",
       " 'ticker\\\\RHI',\n",
       " 'ticker\\\\RJF',\n",
       " 'ticker\\\\RL',\n",
       " 'ticker\\\\RMD',\n",
       " 'ticker\\\\ROK',\n",
       " 'ticker\\\\ROL',\n",
       " 'ticker\\\\ROP',\n",
       " 'ticker\\\\ROST',\n",
       " 'ticker\\\\RSG',\n",
       " 'ticker\\\\RTX',\n",
       " 'ticker\\\\SBAC',\n",
       " 'ticker\\\\SBNY',\n",
       " 'ticker\\\\SBUX',\n",
       " 'ticker\\\\SCHW',\n",
       " 'ticker\\\\SEDG',\n",
       " 'ticker\\\\SEE',\n",
       " 'ticker\\\\SHW',\n",
       " 'ticker\\\\SJM',\n",
       " 'ticker\\\\SLB',\n",
       " 'ticker\\\\SNA',\n",
       " 'ticker\\\\SNPS',\n",
       " 'ticker\\\\SO',\n",
       " 'ticker\\\\SPG',\n",
       " 'ticker\\\\SPGI',\n",
       " 'ticker\\\\SRE',\n",
       " 'ticker\\\\STE',\n",
       " 'ticker\\\\STT',\n",
       " 'ticker\\\\STX',\n",
       " 'ticker\\\\STZ',\n",
       " 'ticker\\\\SWK',\n",
       " 'ticker\\\\SWKS',\n",
       " 'ticker\\\\SYF',\n",
       " 'ticker\\\\SYK',\n",
       " 'ticker\\\\SYY',\n",
       " 'ticker\\\\T',\n",
       " 'ticker\\\\TAP',\n",
       " 'ticker\\\\TDG',\n",
       " 'ticker\\\\TDY',\n",
       " 'ticker\\\\TECH',\n",
       " 'ticker\\\\TEL',\n",
       " 'ticker\\\\TER',\n",
       " 'ticker\\\\TFC',\n",
       " 'ticker\\\\TFX',\n",
       " 'ticker\\\\TGT',\n",
       " 'ticker\\\\TJX',\n",
       " 'ticker\\\\TMO',\n",
       " 'ticker\\\\TMUS',\n",
       " 'ticker\\\\TPR',\n",
       " 'ticker\\\\TRMB',\n",
       " 'ticker\\\\TROW',\n",
       " 'ticker\\\\TRV',\n",
       " 'ticker\\\\TSCO',\n",
       " 'ticker\\\\TSLA',\n",
       " 'ticker\\\\TSN',\n",
       " 'ticker\\\\TT',\n",
       " 'ticker\\\\TTWO',\n",
       " 'ticker\\\\TXN',\n",
       " 'ticker\\\\TXT',\n",
       " 'ticker\\\\TYL',\n",
       " 'ticker\\\\UAL',\n",
       " 'ticker\\\\UDR',\n",
       " 'ticker\\\\UHS',\n",
       " 'ticker\\\\ULTA',\n",
       " 'ticker\\\\UNH',\n",
       " 'ticker\\\\UNP',\n",
       " 'ticker\\\\UPS',\n",
       " 'ticker\\\\URI',\n",
       " 'ticker\\\\USB',\n",
       " 'ticker\\\\V',\n",
       " 'ticker\\\\VFC',\n",
       " 'ticker\\\\VICI',\n",
       " 'ticker\\\\VLO',\n",
       " 'ticker\\\\VMC',\n",
       " 'ticker\\\\VNO',\n",
       " 'ticker\\\\VRSK',\n",
       " 'ticker\\\\VRSN',\n",
       " 'ticker\\\\VRTX',\n",
       " 'ticker\\\\VTR',\n",
       " 'ticker\\\\VTRS',\n",
       " 'ticker\\\\VZ',\n",
       " 'ticker\\\\WAB',\n",
       " 'ticker\\\\WAT',\n",
       " 'ticker\\\\WBA',\n",
       " 'ticker\\\\WBD',\n",
       " 'ticker\\\\WDC',\n",
       " 'ticker\\\\WEC',\n",
       " 'ticker\\\\WELL',\n",
       " 'ticker\\\\WFC',\n",
       " 'ticker\\\\WHR',\n",
       " 'ticker\\\\WM',\n",
       " 'ticker\\\\WMB',\n",
       " 'ticker\\\\WMT',\n",
       " 'ticker\\\\WRB',\n",
       " 'ticker\\\\WRK',\n",
       " 'ticker\\\\WST',\n",
       " 'ticker\\\\WTW',\n",
       " 'ticker\\\\WY',\n",
       " 'ticker\\\\WYNN',\n",
       " 'ticker\\\\XEL',\n",
       " 'ticker\\\\XOM',\n",
       " 'ticker\\\\XRAY',\n",
       " 'ticker\\\\XYL',\n",
       " 'ticker\\\\YUM',\n",
       " 'ticker\\\\ZBH',\n",
       " 'ticker\\\\ZBRA',\n",
       " 'ticker\\\\ZION',\n",
       " 'ticker\\\\ZTS']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  주식 종목의 가격 데이터를 검사하고 데이터의 무결성을 확인하는 함수\n",
    "\n",
    "# data check (OHLCV)\n",
    "def get_tickers_paths():\n",
    "    '''\n",
    "    이 함수는 주식 데이터가 저장된 디렉토리에서 모든 CSV 파일의 경로를 가져옴\n",
    "    '''\n",
    "    tickers_dir = 'data/ticker'\n",
    "    return sorted(glob(f'{tickers_dir}/*'))\n",
    "\n",
    "# save tickers to dataframe\n",
    "def save_tickers_to_df():\n",
    "    '''\n",
    "    이 함수는 주식 데이터를 DataFrame으로 저장하는 역할을 합니다. \n",
    "    모든 CSV 파일의 경로를 가져와서 각 파일을 읽어들인 후, \n",
    "    0 값을 1e-8로 대체하고, NaN 값을 가지는 행이 있는 경우 해당 파일의 경로를 출력합니다. \n",
    "    그런 다음 각 주식의 심볼을 추출하고, \n",
    "    이를 DataFrame으로 저장한 뒤, \n",
    "    'tickers.csv' 파일로 저장합니다.'''\n",
    "    tickers = []\n",
    "    for ticker_path in get_tickers_paths():\n",
    "        df = pd.read_csv(ticker_path)\n",
    "        df = df.replace(0, 1e-8)\n",
    "        if df.isnull().sum().sum() > 0: print(ticker_path)\n",
    "        tickers.append(ticker_path.split('/')[-1].split('.')[0])\n",
    "    pd.DataFrame(np.array(tickers)).to_csv('data/tickers.csv', header='ticker', index=False)\n",
    "\n",
    "# get ticker code list\n",
    "def get_tickers():\n",
    "    '''이 함수는 'tickers.csv' 파일에서 \n",
    "    주식 종목의 심볼을 읽어와 정렬된 리스트로 반환'''\n",
    "    return sorted(list(pd.read_csv('data/tickers.csv').values.flatten()))\n",
    "\n",
    "save_tickers_to_df()\n",
    "get_tickers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features based on the price data\n",
    "\n",
    "가격데이터에 기반해서 특징들을 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train, valid, test period\n",
    "def get_apple(date=False):\n",
    "    if date:\n",
    "        return pd.read_csv('data/ticker/AAPL.csv').set_index('Date')\n",
    "    else:\n",
    "        return pd.read_csv('data/ticker/AAPL.csv')\n",
    "\n",
    "def get_periods(split=False):\n",
    "    # get date index\n",
    "    df_date = get_apple(True)\n",
    "    \n",
    "    # set experiment periods: train, valid, test set\n",
    "    periods = {'train_start':'2021-07-01',\n",
    "               'valid_start':'2022-07-01',\n",
    "               'test_start':'2023-01-03',\n",
    "               'test_end':'2023-08-31'}\n",
    "    \n",
    "    train_start = df_date.index.get_loc(periods['train_start'])\n",
    "    valid_start = df_date.index.get_loc(periods['valid_start'])\n",
    "    test_start = df_date.index.get_loc(periods['test_start'])\n",
    "    test_end = df_date.index.get_loc(periods['test_end'])\n",
    "\n",
    "    # get length of dates\n",
    "    train_period = valid_start - train_start\n",
    "    valid_period = test_start - valid_start\n",
    "    test_period = test_end - test_start + 1\n",
    "    \n",
    "    if split:\n",
    "        return train_period, valid_period, test_period\n",
    "    else:\n",
    "        return train_period + valid_period + test_period\n",
    "\n",
    "period = get_periods()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sklearn.preprocessing (from versions: none)\n",
      "ERROR: No matching distribution found for sklearn.preprocessing\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rladl\\Desktop\\nh투자대회\\그래프_신경망을 활용한 NH 로보어드바이저_해석.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rladl/Desktop/nh%ED%88%AC%EC%9E%90%EB%8C%80%ED%9A%8C/%EA%B7%B8%EB%9E%98%ED%94%84_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84%20%ED%99%9C%EC%9A%A9%ED%95%9C%20NH%20%EB%A1%9C%EB%B3%B4%EC%96%B4%EB%93%9C%EB%B0%94%EC%9D%B4%EC%A0%80_%ED%95%B4%EC%84%9D.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rladl/Desktop/nh%ED%88%AC%EC%9E%90%EB%8C%80%ED%9A%8C/%EA%B7%B8%EB%9E%98%ED%94%84_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84%20%ED%99%9C%EC%9A%A9%ED%95%9C%20NH%20%EB%A1%9C%EB%B3%B4%EC%96%B4%EB%93%9C%EB%B0%94%EC%9D%B4%EC%A0%80_%ED%95%B4%EC%84%9D.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# create eod, mask, base price, ground truth for each stock\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rladl/Desktop/nh%ED%88%AC%EC%9E%90%EB%8C%80%ED%9A%8C/%EA%B7%B8%EB%9E%98%ED%94%84_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84%20%ED%99%9C%EC%9A%A9%ED%95%9C%20NH%20%EB%A1%9C%EB%B3%B4%EC%96%B4%EB%93%9C%EB%B0%94%EC%9D%B4%EC%A0%80_%ED%95%B4%EC%84%9D.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(df, target \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m'\u001b[39m, delay \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m): \u001b[39m# default prediction target is the close price after three days\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rladl/Desktop/nh%ED%88%AC%EC%9E%90%EB%8C%80%ED%9A%8C/%EA%B7%B8%EB%9E%98%ED%94%84_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84%20%ED%99%9C%EC%9A%A9%ED%95%9C%20NH%20%EB%A1%9C%EB%B3%B4%EC%96%B4%EB%93%9C%EB%B0%94%EC%9D%B4%EC%A0%80_%ED%95%B4%EC%84%9D.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# calculate return for ground truth before normalization\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# create eod, mask, base price, ground truth for each stock\n",
    "def transform(df, target = 'Close', delay = 3): # default prediction target is the close price after three days\n",
    "    # calculate return for ground truth before normalization\n",
    "    df['prev'] = df[target].shift(delay, fill_value=1e-8)\n",
    "    df['return'] = (df[target] - df['prev']) / df['prev']\n",
    "    df = df.applymap(lambda x: 1e-8 if x > 1e+8 else x) # return value of delay length is divided 1e-8\n",
    "\n",
    "    # normalize by min-max scaler except return\n",
    "    df = df.tail(period) # double check length\n",
    "    df = df.reset_index().drop(columns = ['Date'])\n",
    "    columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    single_gt = df['return'].to_numpy().copy()\n",
    "    df[columns] = MinMaxScaler().fit_transform(df[columns])\n",
    "\n",
    "    # masking\n",
    "    single_mk = np.ones(len(df))\n",
    "    single_mk[df[df[target] < 1e-8].index.values] = 0\n",
    "    df[columns] = df[columns].applymap(lambda x: 1.1 if x < 1e-8 else x)\n",
    "\n",
    "    single_eod = df[columns].to_numpy()\n",
    "    single_bp = df[target].to_numpy()\n",
    "\n",
    "    return single_eod, single_mk, single_bp, single_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set feature shape\n",
    "tickers = get_tickers()\n",
    "stock_num = len(tickers)\n",
    "feature_size = 5\n",
    "\n",
    "'''\n",
    "    eod: used features for prediction (stock_num, period, feature_size)\n",
    "    mask: zero masking for target (stock_num, period)\n",
    "    base price: target (stock_num, period)\n",
    "    ground truth: return ratio (stock_num, period)\n",
    "'''\n",
    "def create_features():\n",
    "    # init eod, mask, base price, ground truth for all stocks\n",
    "    eod_data = np.zeros((stock_num, period, feature_size))\n",
    "    mask = np.zeros((stock_num, period))\n",
    "    base_price = np.zeros((stock_num, period))\n",
    "    ground_truth = np.zeros((stock_num, period))\n",
    "\n",
    "    # create eod, mask, base price, ground truth for all stocks\n",
    "    tickers_dir = 'data/ticker'\n",
    "    for i, ticker in enumerate(tickers):\n",
    "        df = pd.read_csv(f'{tickers_dir}/{ticker}.csv', index_col = 'Date', parse_dates = True, usecols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume'], na_values=['nan'])\n",
    "        single_eod, single_mk, single_bp, single_gt = transform(df)\n",
    "        eod_data[i] = single_eod\n",
    "        mask[i] = single_mk\n",
    "        base_price[i] = single_bp\n",
    "        ground_truth[i] = single_gt\n",
    "\n",
    "    # save features\n",
    "    feature_dir = 'data/feature'\n",
    "    create_dir(feature_dir)\n",
    "    np.save(f'{feature_dir}/eod.npy', eod_data)\n",
    "    np.save(f'{feature_dir}/mk.npy', mask)\n",
    "    np.save(f'{feature_dir}/bp.npy', base_price)\n",
    "    np.save(f'{feature_dir}/gt.npy', ground_truth)\n",
    "\n",
    "def load_features():\n",
    "    feature_dir = 'data/feature'\n",
    "    eod = np.load(f'{feature_dir}/eod.npy')\n",
    "    mk = np.load(f'{feature_dir}/mk.npy')\n",
    "    bp = np.load(f'{feature_dir}/bp.npy')\n",
    "    gt = np.load(f'{feature_dir}/gt.npy')\n",
    "    return eod, mk, bp, gt\n",
    "\n",
    "\n",
    "# create_features()\n",
    "# load_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/ticker/ticker\\\\A.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rladl\\Desktop\\nh투자대회\\그래프_신경망을 활용한 NH 로보어드바이저_해석.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rladl/Desktop/nh%ED%88%AC%EC%9E%90%EB%8C%80%ED%9A%8C/%EA%B7%B8%EB%9E%98%ED%94%84_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84%20%ED%99%9C%EC%9A%A9%ED%95%9C%20NH%20%EB%A1%9C%EB%B3%B4%EC%96%B4%EB%93%9C%EB%B0%94%EC%9D%B4%EC%A0%80_%ED%95%B4%EC%84%9D.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m create_features()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rladl/Desktop/nh%ED%88%AC%EC%9E%90%EB%8C%80%ED%9A%8C/%EA%B7%B8%EB%9E%98%ED%94%84_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84%20%ED%99%9C%EC%9A%A9%ED%95%9C%20NH%20%EB%A1%9C%EB%B3%B4%EC%96%B4%EB%93%9C%EB%B0%94%EC%9D%B4%EC%A0%80_%ED%95%B4%EC%84%9D.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m load_features()\n",
      "\u001b[1;32mc:\\Users\\rladl\\Desktop\\nh투자대회\\그래프_신경망을 활용한 NH 로보어드바이저_해석.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rladl/Desktop/nh%ED%88%AC%EC%9E%90%EB%8C%80%ED%9A%8C/%EA%B7%B8%EB%9E%98%ED%94%84_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84%20%ED%99%9C%EC%9A%A9%ED%95%9C%20NH%20%EB%A1%9C%EB%B3%B4%EC%96%B4%EB%93%9C%EB%B0%94%EC%9D%B4%EC%A0%80_%ED%95%B4%EC%84%9D.ipynb#X56sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m tickers_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/ticker\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rladl/Desktop/nh%ED%88%AC%EC%9E%90%EB%8C%80%ED%9A%8C/%EA%B7%B8%EB%9E%98%ED%94%84_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84%20%ED%99%9C%EC%9A%A9%ED%95%9C%20NH%20%EB%A1%9C%EB%B3%B4%EC%96%B4%EB%93%9C%EB%B0%94%EC%9D%B4%EC%A0%80_%ED%95%B4%EC%84%9D.ipynb#X56sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, ticker \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tickers):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rladl/Desktop/nh%ED%88%AC%EC%9E%90%EB%8C%80%ED%9A%8C/%EA%B7%B8%EB%9E%98%ED%94%84_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84%20%ED%99%9C%EC%9A%A9%ED%95%9C%20NH%20%EB%A1%9C%EB%B3%B4%EC%96%B4%EB%93%9C%EB%B0%94%EC%9D%B4%EC%A0%80_%ED%95%B4%EC%84%9D.ipynb#X56sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mtickers_dir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mticker\u001b[39m}\u001b[39;49;00m\u001b[39m.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, index_col \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mDate\u001b[39;49m\u001b[39m'\u001b[39;49m, parse_dates \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, usecols \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mDate\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mOpen\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mHigh\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mLow\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mClose\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mVolume\u001b[39;49m\u001b[39m'\u001b[39;49m], na_values\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mnan\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rladl/Desktop/nh%ED%88%AC%EC%9E%90%EB%8C%80%ED%9A%8C/%EA%B7%B8%EB%9E%98%ED%94%84_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84%20%ED%99%9C%EC%9A%A9%ED%95%9C%20NH%20%EB%A1%9C%EB%B3%B4%EC%96%B4%EB%93%9C%EB%B0%94%EC%9D%B4%EC%A0%80_%ED%95%B4%EC%84%9D.ipynb#X56sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     single_eod, single_mk, single_bp, single_gt \u001b[39m=\u001b[39m transform(df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rladl/Desktop/nh%ED%88%AC%EC%9E%90%EB%8C%80%ED%9A%8C/%EA%B7%B8%EB%9E%98%ED%94%84_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84%20%ED%99%9C%EC%9A%A9%ED%95%9C%20NH%20%EB%A1%9C%EB%B3%B4%EC%96%B4%EB%93%9C%EB%B0%94%EC%9D%B4%EC%A0%80_%ED%95%B4%EC%84%9D.ipynb#X56sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     eod_data[i] \u001b[39m=\u001b[39m single_eod\n",
      "File \u001b[1;32mc:\\Users\\rladl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rladl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rladl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\rladl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\rladl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\rladl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\rladl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/ticker/ticker\\\\A.csv'"
     ]
    }
   ],
   "source": [
    "create_features()\n",
    "load_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create hypergraphs based on the sector and industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the dataframe\n",
    "tickers = get_tickers()\n",
    "df = df_valid_start.copy()\n",
    "df.replace('BF.B','BF-B', inplace=True) # remove 'dot'\n",
    "df.replace('BRK.B','BRK-B', inplace=True) # remove 'dot'\n",
    "df = df[df['Symbol'].isin(tickers)] # extract a sub-set of the tickers that have price data\n",
    "df.set_index('Symbol', drop=True, inplace=True) # set index\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# dir setting\n",
    "hyper_dir = 'data/graph/hyper'\n",
    "create_dir(hyper_dir)\n",
    "\n",
    "# save sector & industry\n",
    "df[['GICS Sector']].to_csv(f'{hyper_dir}/sector.csv', index=True)\n",
    "\n",
    "# split industries for each company\n",
    "df['GICS Sub-Industry'].str.split(' & ', expand=True).rename(columns={0:'industry1', 1:'industry2', 2:'industry3'}).to_csv(f'{hyper_dir}/industry.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sector hypergraph\n",
    "df = pd.read_csv(f'{hyper_dir}/sector.csv')\n",
    "pivot_table = pd.crosstab(df['Symbol'], df['GICS Sector'])\n",
    "pivot_table.to_csv(f'{hyper_dir}/sector_hyper.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get industry hypergraph\n",
    "df = pd.read_csv(f'{hyper_dir}/industry.csv')\n",
    "industry_ticker_dict = defaultdict(list)\n",
    "for index, row in df.iterrows():\n",
    "    ticker = row['Symbol']\n",
    "    industries = row[1:]\n",
    "    for industry in industries:\n",
    "        if pd.notna(industry) and industry != \"Other Services\": # the other services is not a industry\n",
    "            industry_ticker_dict[industry].append(ticker)\n",
    "industry_ticker_dict = {k: v for k, v in industry_ticker_dict.items() if len(v) > 1}\n",
    "all_tickers = df['Symbol'].unique()\n",
    "result_df = pd.DataFrame(index=all_tickers, columns=industry_ticker_dict.keys())\n",
    "result_df.index.name = 'Symbol'\n",
    "for industry, tickers in industry_ticker_dict.items():\n",
    "    result_df[industry] = result_df.index.isin(tickers).astype(int)\n",
    "result_df.to_csv(f'{hyper_dir}/industry_hyper.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get complete hypergraph\n",
    "df_sector = pd.read_csv(f'{hyper_dir}/sector_hyper.csv', index_col='Symbol')\n",
    "df_industry = pd.read_csv(f'{hyper_dir}/industry_hyper.csv', index_col='Symbol')\n",
    "result_df = pd.merge(df_sector, df_industry, on='Symbol', how='inner')\n",
    "result_df.to_csv(f'{hyper_dir}/hyper.csv')\n",
    "\n",
    "# load hypergraph\n",
    "def load_hyper_graph():\n",
    "    return pd.read_csv('data/graph/hyper/hyper.csv').set_index('Symbol').values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create simple graphs based on the DTW distance\n",
    "1. Dynamic time warping (DTW)\n",
    "    - DTW efficiently finds the minimum alignment cost between two sequences\n",
    "2. Paper\n",
    "    - https://www.sciencedirect.com/science/article/pii/S003132031000484X\n",
    "3. Open source\n",
    "    - https://pypi.org/project/dtaidistance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph library load\n",
    "import scipy.stats as stats\n",
    "from dtaidistance import dtw_ndim\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set directory\n",
    "simple_dir = 'data/graph/simple'\n",
    "create_dir(simple_dir)\n",
    "\n",
    "distance_dir = 'data/graph/distance'\n",
    "create_dir(distance_dir)\n",
    "\n",
    "# get tickers code and apple\n",
    "tickers_dir = 'data/ticker'\n",
    "tickers = get_tickers() # get ticker code list\n",
    "apple = get_apple()\n",
    "\n",
    "# set hyperparameters\n",
    "window_size = 16\n",
    "sparsity = 0.9\n",
    "\n",
    "# simple merge tickers for distance matrix\n",
    "def merge_multiDim(head):\n",
    "    matrix = []\n",
    "    for ticker in tickers:\n",
    "        df = pd.read_csv(f'{tickers_dir}/{ticker}.csv', index_col = 'Date', usecols=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "        df = df.head(head).tail(window_size)\n",
    "        normalized = stats.zscore(df[['Open','High','Low','Close','Volume']])\n",
    "        matrix.append(normalized)\n",
    "    return np.array(matrix)\n",
    "\n",
    "# get single distance on date\n",
    "def _create_distance(matrix):\n",
    "    distance = []\n",
    "    for s, d1 in enumerate(matrix):\n",
    "        for t, d2 in enumerate(matrix):\n",
    "            dst = dtw_ndim.distance_fast(d1, d2, window = window_size)\n",
    "            distance.append([s, t, dst])\n",
    "    return pd.DataFrame(distance, columns=['source', 'target', 'distance'])\n",
    "\n",
    "# save distance matrix\n",
    "def save_distance(distance, name):\n",
    "    distance.to_csv(f'{distance_dir}/{name}.csv', index=False)\n",
    "\n",
    "# load distance matrix\n",
    "def load_distance(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# get all distances for sliding window\n",
    "def create_distance():\n",
    "    local_apple = apple.tail(period)\n",
    "    print('Create distance matrices...')\n",
    "    for head in tqdm(range(window_size, period + 1)):\n",
    "        date = local_apple.head(head).iloc[-1,].Date\n",
    "        matrix = merge_multiDim(head)\n",
    "        distance = _create_distance(matrix)\n",
    "        save_distance(distance, date)\n",
    "\n",
    "# from distance to adjacency\n",
    "def get_adjacency(path):\n",
    "    distance = load_distance(path)\n",
    "    min_dst = distance['distance'].min()\n",
    "    max_dst = distance['distance'].max()\n",
    "    distance['distance'] = (distance['distance'] - min_dst) / (max_dst - min_dst)\n",
    "    return nx.from_pandas_edgelist(distance, 'source', 'target', 'distance')\n",
    "\n",
    "# dense to sparsity by cutting edges\n",
    "def _create_simple(A): # get threshold\n",
    "    distances = sorted([d for _, _, d in A.edges(data = \"distance\")])\n",
    "    threshold = distances[int(len(distances) * (1 - sparsity))]\n",
    "    A.remove_edges_from([(n1, n2) for n1, n2, d in A.edges(data = \"distance\") if d > threshold])\n",
    "    return A\n",
    "\n",
    "# save graph\n",
    "def save_simple(G, path):\n",
    "    pickle.dump(G, open(path,'wb'))\n",
    "\n",
    "# load graph\n",
    "def load_simple(path):\n",
    "    return pickle.load(open(path, 'rb'))\n",
    "\n",
    "# create simple graphs from the distance matrix\n",
    "def create_simple():\n",
    "    distances = sorted(glob(f'{distance_dir}/*'))\n",
    "    print(\"Create graphs...\")\n",
    "    for dstpath in tqdm(distances):\n",
    "        name = dstpath.split('/')[-1].split('.')[0]\n",
    "        path = f'{simple_dir}/{name}.pickle'\n",
    "        A = get_adjacency(dstpath)\n",
    "        G = _create_simple(A)\n",
    "        save_simple(G, path)\n",
    "\n",
    "# create_distance()\n",
    "# create_simple()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model library load\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import GRU\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import HypergraphConv\n",
    "\n",
    "from scipy import sparse\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split for offset\n",
    "train_period, valid_period, test_period = get_periods(True)\n",
    "window_size = 16\n",
    "delay = 3\n",
    "\n",
    "def get_data_loader(train = False, valid = False, test = False):\n",
    "    # train\n",
    "    if train:\n",
    "        return DataLoader(list(range(train_period - window_size - delay + 1)),\n",
    "                          batch_size=1, shuffle=True)\n",
    "    \n",
    "    # valid\n",
    "    elif valid:\n",
    "        return DataLoader(list(range(train_period - window_size - delay + 1,\n",
    "                          train_period + valid_period - window_size - delay + 1)),\n",
    "                          batch_size=1, shuffle=False)\n",
    "    \n",
    "    # test\n",
    "    elif test:\n",
    "        return DataLoader(list(range(train_period + valid_period - window_size - delay + 1,\n",
    "                          train_period + valid_period + test_period - window_size - delay + 1)),\n",
    "                          batch_size=1, shuffle=False)\n",
    "    \n",
    "    else: return None\n",
    "\n",
    "# set result of test\n",
    "result_dir = '../result'\n",
    "create_dir(f'{result_dir}')\n",
    "\n",
    "model_dir = f'{result_dir}/model'\n",
    "create_dir(f'{model_dir}')\n",
    "\n",
    "ranking_dir = f'{result_dir}/ranking'\n",
    "create_dir(f'{ranking_dir}')\n",
    "\n",
    "test_date = get_apple().tail(test_period)['Date']\n",
    "\n",
    "def save_model(model, name):\n",
    "    torch.save(model, f'{model_dir}/{name}.pt')\n",
    "\n",
    "def load_model(name):\n",
    "    return torch.load(f'{model_dir}/{name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path of simple graphs\n",
    "def get_simple_paths():\n",
    "    simple_dir = '../data/graph/simple'\n",
    "    return sorted(glob(f'{simple_dir}/*'))\n",
    "\n",
    "# get edge index of the simple graph\n",
    "def get_simple_edges(G):\n",
    "    return np.array(list(G.edges)).T\n",
    "\n",
    "# get edge index of the hypergraph\n",
    "def get_hyper_edges():\n",
    "    hypergraph = load_hyper_graph()\n",
    "    inci_sparse = sparse.coo_matrix(hypergraph)\n",
    "    inci_edge = torch_geometric.utils.from_scipy_sparse_matrix(inci_sparse)\n",
    "    return inci_edge[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the input of model given offset\n",
    "eod, mk, bp, gt = load_features()\n",
    "def get_batch(offset, window_size = 16, delay = 3, simple = False, hyper = False):\n",
    "    # batch features\n",
    "    eod_batch = eod[:, offset : offset + window_size, :]\n",
    "    mk_batch = mk[:,  offset : offset + window_size + delay]\n",
    "    mk_batch = np.min(mk_batch, axis=1)\n",
    "    mk_batch = np.expand_dims(mk_batch, axis=1)\n",
    "    bp_batch = np.expand_dims(bp[:, offset + window_size - 1], axis=1)\n",
    "    gt_batch = np.expand_dims(gt[:, offset + window_size + delay - 1], axis=1)\n",
    "    \n",
    "    eod_batch = torch.tensor(eod_batch).float().to(device)\n",
    "    mk_batch = torch.tensor(mk_batch).float().to(device)\n",
    "    bp_batch = torch.tensor(bp_batch).float().to(device)\n",
    "    gt_batch = torch.tensor(gt_batch).float().to(device)\n",
    "    \n",
    "    # batch graphs\n",
    "    if simple:\n",
    "        graph_path = get_simple_paths()[offset]\n",
    "        graph = load_simple(graph_path)\n",
    "        ei_batch = get_simple_edges(graph) # edge of index\n",
    "        ei_batch = ei_batch.clone().detach().long().to(device)\n",
    "        \n",
    "    elif hyper:\n",
    "        ei_batch = get_hyper_edges()\n",
    "        ei_batch = ei_batch.clone().detach().long().to(device)\n",
    "    \n",
    "    else:\n",
    "        ei_batch = None\n",
    "        \n",
    "    return eod_batch, mk_batch, bp_batch, gt_batch, ei_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ranking loss\n",
    "def get_loss(pred, mask, base_price, ground_truth, rel_loss_alpha = 1):\n",
    "    # get absolute loss\n",
    "    return_ratio = torch.div((pred - base_price), base_price)\n",
    "    reg_loss = torch.mean(mask * (return_ratio - ground_truth) ** 2)\n",
    "\n",
    "    # get relative loss\n",
    "    all_ones = torch.ones(pred.size(0), 1).to(device)\n",
    "    pred_rel = torch.matmul(return_ratio, torch.transpose(all_ones, 0, 1)) - torch.matmul(all_ones, torch.transpose(return_ratio, 0, 1))\n",
    "    real_rel = torch.matmul(all_ones, torch.transpose(ground_truth, 0, 1)) - torch.matmul(ground_truth, torch.transpose(all_ones, 0, 1))\n",
    "    mask_rel = torch.matmul(mask, torch.transpose(mask, 0, 1))\n",
    "    rel_loss = torch.mean(F.relu(((pred_rel * real_rel) * mask_rel)))\n",
    "\n",
    "    # get ranking loss\n",
    "    rank_loss = reg_loss + rel_loss_alpha * rel_loss\n",
    "    del mask_rel, real_rel, pred_rel, all_ones\n",
    "    return rank_loss, reg_loss, rel_loss, return_ratio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimizer for model\n",
    "def get_optimizer(model, lr = 1e-3, wd = 5e-4):\n",
    "    if wd is not None:\n",
    "        return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    else:\n",
    "        return torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model and select the model that has lower loss in validation period\n",
    "def train(model, optim, name, simple = False, hyper = False, epochs = 10):\n",
    "    best = np.inf\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\n{epoch+1}/{epochs} epoch')\n",
    "        \n",
    "        # model train\n",
    "        model.train()\n",
    "        tra_loss = 0\n",
    "        data_loader = get_data_loader(train=True)\n",
    "        for offset in tqdm(data_loader): # into batch & each batch offset\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            eod_batch, mk_batch, bp_batch, gt_batch, ei_batch = get_batch(offset, simple = simple, hyper = hyper)\n",
    "            out = model(eod_batch, ei_batch)\n",
    "            batch_loss,_,_,_ = get_loss(out, mk_batch, bp_batch, gt_batch)\n",
    "            \n",
    "            tra_loss += batch_loss.item()\n",
    "            batch_loss.backward()\n",
    "            optim.step()\n",
    "        print('\\tTrain Loss :', round(tra_loss/len(data_loader),6))\n",
    "        \n",
    "        # model validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            data_loader = get_data_loader(valid=True)\n",
    "            for offset in tqdm(data_loader): # into batch & each batch offset\n",
    "                eod_batch, mk_batch, bp_batch, gt_batch, ei_batch = get_batch(offset, simple = simple, hyper = hyper)\n",
    "                out = model(eod_batch, ei_batch)\n",
    "                batch_loss,_,_,_ = get_loss(out, mk_batch, bp_batch, gt_batch)\n",
    "                val_loss += batch_loss.detach().cpu().item()\n",
    "            print('\\tValid Loss :', round(val_loss/len(data_loader),6))\n",
    "            if best > val_loss: # model save to test\n",
    "                best = val_loss\n",
    "                save_model(model, name)\n",
    "                print('model saved')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "def test(model, name, simple = False, hyper = False):\n",
    "    result=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for offset in tqdm(get_data_loader(test=True)):\n",
    "            eod_batch, mk_batch, bp_batch, gt_batch, ei_batch = get_batch(offset, simple = simple, hyper = hyper)\n",
    "            out = model(eod_batch, ei_batch)\n",
    "            _,_,_,rr = get_loss(out, mk_batch, bp_batch, gt_batch)\n",
    "            rr = rr.squeeze(0).squeeze(-1).cpu().detach().numpy()\n",
    "            result.append(rr)\n",
    "\n",
    "    result = np.array(result)\n",
    "    pd.DataFrame(result, index=test_date).to_csv(f'{ranking_dir}/{name}.csv')\n",
    "    del result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple graph-based ranking model\n",
    "1. Graph attention network (GAT)\n",
    "    - GAT is an advanced graph neural network to learn the node repesentations using the relations between nodes in the simple graph\n",
    "    - GAT uses the self-attention mechanism to assign different weights based on the importance of the neighboring nodes\n",
    "2. Paper\n",
    "    - P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, ‘‘Graph attention networks,’’ in Proc. ICLR, 2018, pp. 1–12.\n",
    "    - https://arxiv.org/abs/1710.10903\n",
    "3. Open source\n",
    "    - https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html#torch-geometric-nn-conv-gatconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture define for the simple graph\n",
    "class SGM(torch.nn.Module):\n",
    "    def __init__(self, window_size = 16, feature_size = 5, hidden_size = 32, out_size = 16, head_size = 4):\n",
    "        super().__init__()\n",
    "        # temporal modeling\n",
    "        self.gru = GRU(feature_size, hidden_size, 1, batch_first = True)\n",
    "        \n",
    "        # relational modeling\n",
    "        self.conv1 = GATConv(window_size * hidden_size, window_size * hidden_size, head_size, dropout=0.5)\n",
    "        self.conv2 = GATConv(window_size * hidden_size * head_size, out_size, heads=1, concat=False, dropout=0.5)\n",
    "        \n",
    "        # dense layer for ranking score\n",
    "        self.linear = torch.nn.Linear(out_size, 1)\n",
    "\n",
    "    def forward(self, src, edge_index): # stock_num, window_size, feature_size\n",
    "        out, _ = self.gru(src) # stock_num, window_size, hidden_size\n",
    "        out = out.reshape(src.size(0), -1) # stock_num, window_size * hidden_size\n",
    "        out = F.elu(self.conv1(out, edge_index)) # stock_num, window_size * hidden_size * head_size\n",
    "        out = F.elu(self.conv2(out, edge_index)) # stock_num, out_size\n",
    "        out = F.leaky_relu(self.linear(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the simple graph-based ranking models\n",
    "for t in range(10):\n",
    "    model = SGM(16, 5, 32, 16, 4).to(device)\n",
    "    optim = get_optimizer(model)\n",
    "    train(model, optim, f'simple{t}', simple = True)\n",
    "    test(load_model(f'simple{t}'), f'simple{t}', simple = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HConv model\n",
    "1. Hypergraph Convolution and Hypergraph Atterntion (HConv)\n",
    "    - HConv can extract the relational features from hypergraphs.\n",
    "    - The hypergraph represents collective relations among stocks.\n",
    "2. Paper\n",
    "    - S. Bai, F. Zhang, and P. H. S. Torr, ‘‘Hypergraph convolution and hypergraph attention,’’ Pattern Recognit., vol. 110, Feb. 2021, Art. no. 107637\n",
    "    - https://arxiv.org/pdf/1901.08150.pdf\n",
    "3. Open source\n",
    "    - https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.HypergraphConv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture define for the hypergraph\n",
    "class HGM(torch.nn.Module):\n",
    "    def __init__(self, window_size = 16, feature_size = 5, hidden_size = 32, out_size = 16, head_size = 4):\n",
    "        super().__init__()\n",
    "        # temporal modeling\n",
    "        self.gru = GRU(feature_size, hidden_size, 1, batch_first = True)\n",
    "        \n",
    "        # relational modeling\n",
    "        self.conv1 = HypergraphConv(window_size * hidden_size, hidden_size,\n",
    "                                    use_attention=True, heads=head_size, concat=True, dropout=0.5)\n",
    "        self.conv2 = HypergraphConv(hidden_size * head_size, out_size,\n",
    "                                    heads=1, concat=False, dropout=0.5)\n",
    "        \n",
    "        # dense layer for ranking score\n",
    "        self.linear = torch.nn.Linear(out_size, 1)\n",
    "        \n",
    "    def get_hyperedge_attr(self, x, edge_index):\n",
    "        src = x[edge_index[0]].T\n",
    "        idx = edge_index[1]\n",
    "        return scatter_add(src, idx).T\n",
    "    \n",
    "    def forward(self, src, edge_index):\n",
    "        \n",
    "        out, _ = self.gru(src) # stock_num, window_size, hidden_size\n",
    "        out = out.reshape(out.size(0), -1) # stock_num, window_size * hidden_size\n",
    "        out = F.elu(self.conv1(out, edge_index,\n",
    "                               hyperedge_weight = None,\n",
    "                               hyperedge_attr = self.get_hyperedge_attr(out, edge_index)))\n",
    "        # stock_num, out_size\n",
    "        out = F.elu(self.conv2(out, edge_index,\n",
    "                               hyperedge_weight = None,\n",
    "                               hyperedge_attr = self.get_hyperedge_attr(out, edge_index)))\n",
    "        out = F.leaky_relu(self.linear(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the hypergraph-based ranking models\n",
    "for t in range(10):\n",
    "    model = HGM(16, 5, 32, 16, 4).to(device)\n",
    "    optim = get_optimizer(model)\n",
    "    train(model, optim, f'hyper{t}', hyper = True)\n",
    "    test(load_model(f'hyper{t}'), f'hyper{t}', hyper = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
